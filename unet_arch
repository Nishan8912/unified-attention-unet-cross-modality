digraph {
	graph [size="34.05,34.05"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140317273003808 [label="
 (1, 1, 128, 128)" fillcolor=darkolivegreen1]
	140317271166880 [label=ConvolutionBackward0]
	140317271166688 -> 140317271166880
	140317271166688 [label=LeakyReluBackward1]
	140317271166400 -> 140317271166688
	140317271166400 [label=CudnnBatchNormBackward0]
	140317271166256 -> 140317271166400
	140317271166256 [label=ConvolutionBackward0]
	140317273312176 -> 140317271166256
	140317273312176 [label=LeakyReluBackward1]
	140317273067728 -> 140317273312176
	140317273067728 [label=CudnnBatchNormBackward0]
	140317273067824 -> 140317273067728
	140317273067824 [label=ConvolutionBackward0]
	140317273068016 -> 140317273067824
	140317273068016 [label=CatBackward0]
	140317273068160 -> 140317273068016
	140317273068160 [label=ConvolutionBackward0]
	140317273068304 -> 140317273068160
	140317273068304 [label=LeakyReluBackward1]
	140317273068496 -> 140317273068304
	140317273068496 [label=CudnnBatchNormBackward0]
	140317273068592 -> 140317273068496
	140317273068592 [label=ConvolutionBackward0]
	140317273068784 -> 140317273068592
	140317273068784 [label=LeakyReluBackward1]
	140317273068928 -> 140317273068784
	140317273068928 [label=CudnnBatchNormBackward0]
	140317273069024 -> 140317273068928
	140317273069024 [label=ConvolutionBackward0]
	140317273069216 -> 140317273069024
	140317273069216 [label=CatBackward0]
	140317273069360 -> 140317273069216
	140317273069360 [label=ConvolutionBackward0]
	140317273069504 -> 140317273069360
	140317273069504 [label=MulBackward0]
	140317273069696 -> 140317273069504
	140317273069696 [label=LeakyReluBackward1]
	140317273069792 -> 140317273069696
	140317273069792 [label=CudnnBatchNormBackward0]
	140317273069888 -> 140317273069792
	140317273069888 [label=ConvolutionBackward0]
	140317273070080 -> 140317273069888
	140317273070080 [label=LeakyReluBackward1]
	140317273070224 -> 140317273070080
	140317273070224 [label=CudnnBatchNormBackward0]
	140317273070320 -> 140317273070224
	140317273070320 [label=ConvolutionBackward0]
	140317273070512 -> 140317273070320
	140317273070512 [label=MaxPool2DWithIndicesBackward0]
	140317273069312 -> 140317273070512
	140317273069312 [label=LeakyReluBackward1]
	140317273070704 -> 140317273069312
	140317273070704 [label=CudnnBatchNormBackward0]
	140317273070800 -> 140317273070704
	140317273070800 [label=ConvolutionBackward0]
	140317273070992 -> 140317273070800
	140317273070992 [label=LeakyReluBackward1]
	140317273071136 -> 140317273070992
	140317273071136 [label=CudnnBatchNormBackward0]
	140317273071232 -> 140317273071136
	140317273071232 [label=ConvolutionBackward0]
	140317273071424 -> 140317273071232
	140317273071424 [label=MaxPool2DWithIndicesBackward0]
	140317273068112 -> 140317273071424
	140317273068112 [label=LeakyReluBackward1]
	140317273071472 -> 140317273068112
	140317273071472 [label=CudnnBatchNormBackward0]
	140317273014432 -> 140317273071472
	140317273014432 [label=ConvolutionBackward0]
	140317273014624 -> 140317273014432
	140317273014624 [label=LeakyReluBackward1]
	140317273014768 -> 140317273014624
	140317273014768 [label=CudnnBatchNormBackward0]
	140317273014864 -> 140317273014768
	140317273014864 [label=ConvolutionBackward0]
	140317273015056 -> 140317273014864
	140317387691808 [label="
 (1, 4, 128, 128)" fillcolor=lightblue]
	140317387691808 -> 140317273015056
	140317273015056 [label=AccumulateGrad]
	140317273015008 -> 140317273014864
	140317387692448 [label="encoder1.block.0.weight
 (64, 4, 3, 3)" fillcolor=lightblue]
	140317387692448 -> 140317273015008
	140317273015008 [label=AccumulateGrad]
	140317273014816 -> 140317273014768
	140317387691648 [label="encoder1.block.1.weight
 (64)" fillcolor=lightblue]
	140317387691648 -> 140317273014816
	140317273014816 [label=AccumulateGrad]
	140317273014672 -> 140317273014768
	140317387692608 [label="encoder1.block.1.bias
 (64)" fillcolor=lightblue]
	140317387692608 -> 140317273014672
	140317273014672 [label=AccumulateGrad]
	140317273014576 -> 140317273014432
	140317271544512 [label="encoder1.block.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140317271544512 -> 140317273014576
	140317273014576 [label=AccumulateGrad]
	140317273014384 -> 140317273071472
	140317271543952 [label="encoder1.block.4.weight
 (64)" fillcolor=lightblue]
	140317271543952 -> 140317273014384
	140317273014384 [label=AccumulateGrad]
	140317273014336 -> 140317273071472
	140317271544432 [label="encoder1.block.4.bias
 (64)" fillcolor=lightblue]
	140317271544432 -> 140317273014336
	140317273014336 [label=AccumulateGrad]
	140317273071376 -> 140317273071232
	140317387686112 [label="encoder2.block.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	140317387686112 -> 140317273071376
	140317273071376 [label=AccumulateGrad]
	140317273071184 -> 140317273071136
	140317387686272 [label="encoder2.block.1.weight
 (128)" fillcolor=lightblue]
	140317387686272 -> 140317273071184
	140317273071184 [label=AccumulateGrad]
	140317273071040 -> 140317273071136
	140317387686032 [label="encoder2.block.1.bias
 (128)" fillcolor=lightblue]
	140317387686032 -> 140317273071040
	140317273071040 [label=AccumulateGrad]
	140317273070944 -> 140317273070800
	140317387687232 [label="encoder2.block.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140317387687232 -> 140317273070944
	140317273070944 [label=AccumulateGrad]
	140317273070752 -> 140317273070704
	140317387688672 [label="encoder2.block.4.weight
 (128)" fillcolor=lightblue]
	140317387688672 -> 140317273070752
	140317273070752 [label=AccumulateGrad]
	140317273070608 -> 140317273070704
	140317388829296 [label="encoder2.block.4.bias
 (128)" fillcolor=lightblue]
	140317388829296 -> 140317273070608
	140317273070608 [label=AccumulateGrad]
	140317273070464 -> 140317273070320
	140317388779408 [label="bottleneck.block.0.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	140317388779408 -> 140317273070464
	140317273070464 [label=AccumulateGrad]
	140317273070272 -> 140317273070224
	140317388777808 [label="bottleneck.block.1.weight
 (256)" fillcolor=lightblue]
	140317388777808 -> 140317273070272
	140317273070272 [label=AccumulateGrad]
	140317273070128 -> 140317273070224
	140317388778768 [label="bottleneck.block.1.bias
 (256)" fillcolor=lightblue]
	140317388778768 -> 140317273070128
	140317273070128 [label=AccumulateGrad]
	140317273070032 -> 140317273069888
	140317271963904 [label="bottleneck.block.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140317271963904 -> 140317273070032
	140317273070032 [label=AccumulateGrad]
	140317273069840 -> 140317273069792
	140317271964624 [label="bottleneck.block.4.weight
 (256)" fillcolor=lightblue]
	140317271964624 -> 140317273069840
	140317273069840 [label=AccumulateGrad]
	140317273069600 -> 140317273069792
	140317271965024 [label="bottleneck.block.4.bias
 (256)" fillcolor=lightblue]
	140317271965024 -> 140317273069600
	140317273069600 [label=AccumulateGrad]
	140317273069456 -> 140317273069360
	140317271941424 [label="up2.weight
 (256, 128, 2, 2)" fillcolor=lightblue]
	140317271941424 -> 140317273069456
	140317273069456 [label=AccumulateGrad]
	140317273069408 -> 140317273069360
	140317271941264 [label="up2.bias
 (128)" fillcolor=lightblue]
	140317271941264 -> 140317273069408
	140317273069408 [label=AccumulateGrad]
	140317273069312 -> 140317273069216
	140317273069168 -> 140317273069024
	140317271941344 [label="decoder2.block.0.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	140317271941344 -> 140317273069168
	140317273069168 [label=AccumulateGrad]
	140317273068976 -> 140317273068928
	140317271941584 [label="decoder2.block.1.weight
 (128)" fillcolor=lightblue]
	140317271941584 -> 140317273068976
	140317273068976 [label=AccumulateGrad]
	140317273068832 -> 140317273068928
	140317271941664 [label="decoder2.block.1.bias
 (128)" fillcolor=lightblue]
	140317271941664 -> 140317273068832
	140317273068832 [label=AccumulateGrad]
	140317273068736 -> 140317273068592
	140317271942064 [label="decoder2.block.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140317271942064 -> 140317273068736
	140317273068736 [label=AccumulateGrad]
	140317273068544 -> 140317273068496
	140317271942144 [label="decoder2.block.4.weight
 (128)" fillcolor=lightblue]
	140317271942144 -> 140317273068544
	140317273068544 [label=AccumulateGrad]
	140317273068400 -> 140317273068496
	140317271941984 [label="decoder2.block.4.bias
 (128)" fillcolor=lightblue]
	140317271941984 -> 140317273068400
	140317273068400 [label=AccumulateGrad]
	140317273068256 -> 140317273068160
	140317271942624 [label="up1.weight
 (128, 64, 2, 2)" fillcolor=lightblue]
	140317271942624 -> 140317273068256
	140317273068256 [label=AccumulateGrad]
	140317273068208 -> 140317273068160
	140317271942704 [label="up1.bias
 (64)" fillcolor=lightblue]
	140317271942704 -> 140317273068208
	140317273068208 [label=AccumulateGrad]
	140317273068112 -> 140317273068016
	140317273067968 -> 140317273067824
	140317271942864 [label="decoder1.block.0.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	140317271942864 -> 140317273067968
	140317273067968 [label=AccumulateGrad]
	140317273067776 -> 140317273067728
	140317271942944 [label="decoder1.block.1.weight
 (64)" fillcolor=lightblue]
	140317271942944 -> 140317273067776
	140317273067776 [label=AccumulateGrad]
	140317273067632 -> 140317273067728
	140317271943024 [label="decoder1.block.1.bias
 (64)" fillcolor=lightblue]
	140317271943024 -> 140317273067632
	140317273067632 [label=AccumulateGrad]
	140317271166160 -> 140317271166256
	140317271943424 [label="decoder1.block.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140317271943424 -> 140317271166160
	140317271166160 [label=AccumulateGrad]
	140317271166352 -> 140317271166400
	140317271943504 [label="decoder1.block.4.weight
 (64)" fillcolor=lightblue]
	140317271943504 -> 140317271166352
	140317271166352 [label=AccumulateGrad]
	140317271166448 -> 140317271166400
	140317271943584 [label="decoder1.block.4.bias
 (64)" fillcolor=lightblue]
	140317271943584 -> 140317271166448
	140317271166448 [label=AccumulateGrad]
	140317271166496 -> 140317271166880
	140317271943984 [label="final_conv.weight
 (1, 64, 1, 1)" fillcolor=lightblue]
	140317271943984 -> 140317271166496
	140317271166496 [label=AccumulateGrad]
	140317271166640 -> 140317271166880
	140317271944064 [label="final_conv.bias
 (1)" fillcolor=lightblue]
	140317271944064 -> 140317271166640
	140317271166640 [label=AccumulateGrad]
	140317271166880 -> 140317273003808
}
