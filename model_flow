digraph {
	graph [size="54.15,54.15"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	139759839991184 [label="
 (1, 1, 128, 128)" fillcolor=darkolivegreen1]
	139777017828304 [label=ConvolutionBackward0]
	139777017829888 -> 139777017828304
	139777017829888 [label=LeakyReluBackward1]
	139777017831232 -> 139777017829888
	139777017831232 [label=CudnnBatchNormBackward0]
	139777017827920 -> 139777017831232
	139777017827920 [label=ConvolutionBackward0]
	139777017828688 -> 139777017827920
	139777017828688 [label=LeakyReluBackward1]
	139777017830464 -> 139777017828688
	139777017830464 [label=CudnnBatchNormBackward0]
	139777017830128 -> 139777017830464
	139777017830128 [label=ConvolutionBackward0]
	139777017829312 -> 139777017830128
	139777017829312 [label=CatBackward0]
	139777017829408 -> 139777017829312
	139777017829408 [label=ConvolutionBackward0]
	139777017830704 -> 139777017829408
	139777017830704 [label=LeakyReluBackward1]
	139777017829024 -> 139777017830704
	139777017829024 [label=CudnnBatchNormBackward0]
	139777017831184 -> 139777017829024
	139777017831184 [label=ConvolutionBackward0]
	139777017827776 -> 139777017831184
	139777017827776 [label=LeakyReluBackward1]
	139777017828544 -> 139777017827776
	139777017828544 [label=CudnnBatchNormBackward0]
	139777017829072 -> 139777017828544
	139777017829072 [label=ConvolutionBackward0]
	139777017829264 -> 139777017829072
	139777017829264 [label=CatBackward0]
	139777017829792 -> 139777017829264
	139777017829792 [label=ConvolutionBackward0]
	139777017828496 -> 139777017829792
	139777017828496 [label=LeakyReluBackward1]
	139777017830992 -> 139777017828496
	139777017830992 [label=CudnnBatchNormBackward0]
	139777019577248 -> 139777017830992
	139777019577248 [label=ConvolutionBackward0]
	139777019578784 -> 139777019577248
	139777019578784 [label=LeakyReluBackward1]
	139777019579456 -> 139777019578784
	139777019579456 [label=CudnnBatchNormBackward0]
	139777019576720 -> 139777019579456
	139777019576720 [label=ConvolutionBackward0]
	139777019577152 -> 139777019576720
	139777019577152 [label=MaxPool2DWithIndicesBackward0]
	139777019576576 -> 139777019577152
	139777019576576 [label=LeakyReluBackward1]
	139777019578592 -> 139777019576576
	139777019578592 [label=CudnnBatchNormBackward0]
	139777019577968 -> 139777019578592
	139777019577968 [label=ConvolutionBackward0]
	139777019579168 -> 139777019577968
	139777019579168 [label=LeakyReluBackward1]
	139777019579264 -> 139777019579168
	139777019579264 [label=CudnnBatchNormBackward0]
	139777019579840 -> 139777019579264
	139777019579840 [label=ConvolutionBackward0]
	139777019578304 -> 139777019579840
	139777019578304 [label=MaxPool2DWithIndicesBackward0]
	139777019579312 -> 139777019578304
	139777019579312 [label=LeakyReluBackward1]
	139777019578688 -> 139777019579312
	139777019578688 [label=CudnnBatchNormBackward0]
	139777019578448 -> 139777019578688
	139777019578448 [label=ConvolutionBackward0]
	139777019576768 -> 139777019578448
	139777019576768 [label=LeakyReluBackward1]
	139777019577824 -> 139777019576768
	139777019577824 [label=CudnnBatchNormBackward0]
	139777019578112 -> 139777019577824
	139777019578112 [label=ConvolutionBackward0]
	139777019576912 -> 139777019578112
	139777018144624 [label="encoder1.block.0.weight
 (64, 4, 3, 3)" fillcolor=lightblue]
	139777018144624 -> 139777019576912
	139777019576912 [label=AccumulateGrad]
	139777019580032 -> 139777019577824
	139758602480992 [label="encoder1.block.1.weight
 (64)" fillcolor=lightblue]
	139758602480992 -> 139777019580032
	139777019580032 [label=AccumulateGrad]
	139777019579552 -> 139777019577824
	139762526915296 [label="encoder1.block.1.bias
 (64)" fillcolor=lightblue]
	139762526915296 -> 139777019579552
	139777019579552 [label=AccumulateGrad]
	139777019576528 -> 139777019578448
	139762520307088 [label="encoder1.block.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139762520307088 -> 139777019576528
	139777019576528 [label=AccumulateGrad]
	139777019579024 -> 139777019578688
	139777018143424 [label="encoder1.block.4.weight
 (64)" fillcolor=lightblue]
	139777018143424 -> 139777019579024
	139777019579024 [label=AccumulateGrad]
	139777019578496 -> 139777019578688
	139777018144304 [label="encoder1.block.4.bias
 (64)" fillcolor=lightblue]
	139777018144304 -> 139777019578496
	139777019578496 [label=AccumulateGrad]
	139777019578928 -> 139777019579840
	139762866661152 [label="encoder2.block.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	139762866661152 -> 139777019578928
	139777019578928 [label=AccumulateGrad]
	139777019578400 -> 139777019579264
	139762866659712 [label="encoder2.block.1.weight
 (128)" fillcolor=lightblue]
	139762866659712 -> 139777019578400
	139777019578400 [label=AccumulateGrad]
	139777019580224 -> 139777019579264
	139762866659632 [label="encoder2.block.1.bias
 (128)" fillcolor=lightblue]
	139762866659632 -> 139777019580224
	139777019580224 [label=AccumulateGrad]
	139777019577104 -> 139777019577968
	139762866663072 [label="encoder2.block.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139762866663072 -> 139777019577104
	139777019577104 [label=AccumulateGrad]
	139777019578160 -> 139777019578592
	139762866659392 [label="encoder2.block.4.weight
 (128)" fillcolor=lightblue]
	139762866659392 -> 139777019578160
	139777019578160 [label=AccumulateGrad]
	139777019579744 -> 139777019578592
	139762866660032 [label="encoder2.block.4.bias
 (128)" fillcolor=lightblue]
	139762866660032 -> 139777019579744
	139777019579744 [label=AccumulateGrad]
	139777019580272 -> 139777019576720
	139762866663152 [label="bottleneck.block.0.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	139762866663152 -> 139777019580272
	139777019580272 [label=AccumulateGrad]
	139777019577488 -> 139777019579456
	139762866663232 [label="bottleneck.block.1.weight
 (256)" fillcolor=lightblue]
	139762866663232 -> 139777019577488
	139777019577488 [label=AccumulateGrad]
	139777019579696 -> 139777019579456
	139762866662672 [label="bottleneck.block.1.bias
 (256)" fillcolor=lightblue]
	139762866662672 -> 139777019579696
	139777019579696 [label=AccumulateGrad]
	139777019576672 -> 139777019577248
	139777018221008 [label="bottleneck.block.3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139777018221008 -> 139777019576672
	139777019576672 [label=AccumulateGrad]
	139777019577872 -> 139777017830992
	139777017147200 [label="bottleneck.block.4.weight
 (256)" fillcolor=lightblue]
	139777017147200 -> 139777019577872
	139777019577872 [label=AccumulateGrad]
	139777019577776 -> 139777017830992
	139777017144400 [label="bottleneck.block.4.bias
 (256)" fillcolor=lightblue]
	139777017144400 -> 139777019577776
	139777019577776 [label=AccumulateGrad]
	139777017828640 -> 139777017829792
	139777017147280 [label="up2.weight
 (256, 128, 2, 2)" fillcolor=lightblue]
	139777017147280 -> 139777017828640
	139777017828640 [label=AccumulateGrad]
	139777017830176 -> 139777017829792
	139777017589904 [label="up2.bias
 (128)" fillcolor=lightblue]
	139777017589904 -> 139777017830176
	139777017830176 [label=AccumulateGrad]
	139777017830320 -> 139777017829264
	139777017830320 [label=MulBackward0]
	139777019576576 -> 139777017830320
	139777017830368 -> 139777017830320
	139777017830368 [label=SigmoidBackward0]
	139777019576432 -> 139777017830368
	139777019576432 [label=CudnnBatchNormBackward0]
	139777019578736 -> 139777019576432
	139777019578736 [label=ConvolutionBackward0]
	139777019579648 -> 139777019578736
	139777019579648 [label=ReluBackward0]
	139777019577296 -> 139777019579648
	139777019577296 [label=AddBackward0]
	139777019576480 -> 139777019577296
	139777019576480 [label=CudnnBatchNormBackward0]
	139777019579072 -> 139777019576480
	139777019579072 [label=ConvolutionBackward0]
	139777017829792 -> 139777019579072
	139777019578352 -> 139777019579072
	139777017181104 [label="att2.W_g.0.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	139777017181104 -> 139777019578352
	139777019578352 [label=AccumulateGrad]
	139777019579216 -> 139777019579072
	139777017183824 [label="att2.W_g.0.bias
 (64)" fillcolor=lightblue]
	139777017183824 -> 139777019579216
	139777019579216 [label=AccumulateGrad]
	139777019578976 -> 139777019576480
	139777017183744 [label="att2.W_g.1.weight
 (64)" fillcolor=lightblue]
	139777017183744 -> 139777019578976
	139777019578976 [label=AccumulateGrad]
	139777019580320 -> 139777019576480
	139777017181184 [label="att2.W_g.1.bias
 (64)" fillcolor=lightblue]
	139777017181184 -> 139777019580320
	139777019580320 [label=AccumulateGrad]
	139777019577008 -> 139777019577296
	139777019577008 [label=CudnnBatchNormBackward0]
	139777019577728 -> 139777019577008
	139777019577728 [label=ConvolutionBackward0]
	139777019576576 -> 139777019577728
	139777019578064 -> 139777019577728
	139777017194992 [label="att2.W_x.0.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	139777017194992 -> 139777019578064
	139777019578064 [label=AccumulateGrad]
	139777019578880 -> 139777019577728
	139777017194512 [label="att2.W_x.0.bias
 (64)" fillcolor=lightblue]
	139777017194512 -> 139777019578880
	139777019578880 [label=AccumulateGrad]
	139777019578208 -> 139777019577008
	139777017193072 [label="att2.W_x.1.weight
 (64)" fillcolor=lightblue]
	139777017193072 -> 139777019578208
	139777019578208 [label=AccumulateGrad]
	139777019577392 -> 139777019577008
	139777017192832 [label="att2.W_x.1.bias
 (64)" fillcolor=lightblue]
	139777017192832 -> 139777019577392
	139777019577392 [label=AccumulateGrad]
	139777019577440 -> 139777019578736
	139777017194112 [label="att2.psi.0.weight
 (1, 64, 1, 1)" fillcolor=lightblue]
	139777017194112 -> 139777019577440
	139777019577440 [label=AccumulateGrad]
	139777019577344 -> 139777019578736
	139777017195152 [label="att2.psi.0.bias
 (1)" fillcolor=lightblue]
	139777017195152 -> 139777019577344
	139777019577344 [label=AccumulateGrad]
	139777019578256 -> 139777019576432
	139777017193152 [label="att2.psi.1.weight
 (1)" fillcolor=lightblue]
	139777017193152 -> 139777019578256
	139777019578256 [label=AccumulateGrad]
	139777019579504 -> 139777019576432
	139777017194912 [label="att2.psi.1.bias
 (1)" fillcolor=lightblue]
	139777017194912 -> 139777019579504
	139777019579504 [label=AccumulateGrad]
	139777017829168 -> 139777017829072
	139777017195232 [label="decoder2.block.0.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	139777017195232 -> 139777017829168
	139777017829168 [label=AccumulateGrad]
	139777017829456 -> 139777017828544
	139777017195312 [label="decoder2.block.1.weight
 (128)" fillcolor=lightblue]
	139777017195312 -> 139777017829456
	139777017829456 [label=AccumulateGrad]
	139777017831280 -> 139777017828544
	139777017195392 [label="decoder2.block.1.bias
 (128)" fillcolor=lightblue]
	139777017195392 -> 139777017831280
	139777017831280 [label=AccumulateGrad]
	139777017831136 -> 139777017831184
	139777017195792 [label="decoder2.block.3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139777017195792 -> 139777017831136
	139777017831136 [label=AccumulateGrad]
	139777017829984 -> 139777017829024
	139777017195872 [label="decoder2.block.4.weight
 (128)" fillcolor=lightblue]
	139777017195872 -> 139777017829984
	139777017829984 [label=AccumulateGrad]
	139777017828208 -> 139777017829024
	139777017195952 [label="decoder2.block.4.bias
 (128)" fillcolor=lightblue]
	139777017195952 -> 139777017828208
	139777017828208 [label=AccumulateGrad]
	139777017830032 -> 139777017829408
	139777017196352 [label="up1.weight
 (128, 64, 2, 2)" fillcolor=lightblue]
	139777017196352 -> 139777017830032
	139777017830032 [label=AccumulateGrad]
	139777017828928 -> 139777017829408
	139777017196432 [label="up1.bias
 (64)" fillcolor=lightblue]
	139777017196432 -> 139777017828928
	139777017828928 [label=AccumulateGrad]
	139777017831040 -> 139777017829312
	139777017831040 [label=MulBackward0]
	139777019579312 -> 139777017831040
	139777017829552 -> 139777017831040
	139777017829552 [label=SigmoidBackward0]
	139777017827968 -> 139777017829552
	139777017827968 [label=CudnnBatchNormBackward0]
	139777017830560 -> 139777017827968
	139777017830560 [label=ConvolutionBackward0]
	139777017827488 -> 139777017830560
	139777017827488 [label=ReluBackward0]
	139777019578832 -> 139777017827488
	139777019578832 [label=AddBackward0]
	139777019579120 -> 139777019578832
	139777019579120 [label=CudnnBatchNormBackward0]
	139777019580176 -> 139777019579120
	139777019580176 [label=ConvolutionBackward0]
	139777017829408 -> 139777019580176
	139777019580080 -> 139777019580176
	139762938171536 [label="att1.W_g.0.weight
 (32, 64, 1, 1)" fillcolor=lightblue]
	139762938171536 -> 139777019580080
	139777019580080 [label=AccumulateGrad]
	139777137634848 -> 139777019580176
	139762938171616 [label="att1.W_g.0.bias
 (32)" fillcolor=lightblue]
	139762938171616 -> 139777137634848
	139777137634848 [label=AccumulateGrad]
	139777019576624 -> 139777019579120
	139762938171696 [label="att1.W_g.1.weight
 (32)" fillcolor=lightblue]
	139762938171696 -> 139777019576624
	139777019576624 [label=AccumulateGrad]
	139777019576864 -> 139777019579120
	139762938171776 [label="att1.W_g.1.bias
 (32)" fillcolor=lightblue]
	139762938171776 -> 139777019576864
	139777019576864 [label=AccumulateGrad]
	139777019577056 -> 139777019578832
	139777019577056 [label=CudnnBatchNormBackward0]
	139777019576816 -> 139777019577056
	139777019576816 [label=ConvolutionBackward0]
	139777019579312 -> 139777019576816
	139777137632640 -> 139777019576816
	139762938172176 [label="att1.W_x.0.weight
 (32, 64, 1, 1)" fillcolor=lightblue]
	139762938172176 -> 139777137632640
	139777137632640 [label=AccumulateGrad]
	139777137632352 -> 139777019576816
	139762938172256 [label="att1.W_x.0.bias
 (32)" fillcolor=lightblue]
	139762938172256 -> 139777137632352
	139777137632352 [label=AccumulateGrad]
	139777137634896 -> 139777019577056
	139762938172336 [label="att1.W_x.1.weight
 (32)" fillcolor=lightblue]
	139762938172336 -> 139777137634896
	139777137634896 [label=AccumulateGrad]
	139777137633888 -> 139777019577056
	139762938172416 [label="att1.W_x.1.bias
 (32)" fillcolor=lightblue]
	139762938172416 -> 139777137633888
	139777137633888 [label=AccumulateGrad]
	139777019579360 -> 139777017830560
	139762938172816 [label="att1.psi.0.weight
 (1, 32, 1, 1)" fillcolor=lightblue]
	139762938172816 -> 139777019579360
	139777019579360 [label=AccumulateGrad]
	139777019577536 -> 139777017830560
	139762938172896 [label="att1.psi.0.bias
 (1)" fillcolor=lightblue]
	139762938172896 -> 139777019577536
	139777019577536 [label=AccumulateGrad]
	139777017829216 -> 139777017827968
	139762938172976 [label="att1.psi.1.weight
 (1)" fillcolor=lightblue]
	139762938172976 -> 139777017829216
	139777017829216 [label=AccumulateGrad]
	139777017827824 -> 139777017827968
	139762938173056 [label="att1.psi.1.bias
 (1)" fillcolor=lightblue]
	139762938173056 -> 139777017827824
	139777017827824 [label=AccumulateGrad]
	139777017828784 -> 139777017830128
	139762938173456 [label="decoder1.block.0.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	139762938173456 -> 139777017828784
	139777017828784 [label=AccumulateGrad]
	139777017827680 -> 139777017830464
	139762938173536 [label="decoder1.block.1.weight
 (64)" fillcolor=lightblue]
	139762938173536 -> 139777017827680
	139777017827680 [label=AccumulateGrad]
	139777017828592 -> 139777017830464
	139762938173616 [label="decoder1.block.1.bias
 (64)" fillcolor=lightblue]
	139762938173616 -> 139777017828592
	139777017828592 [label=AccumulateGrad]
	139777017830896 -> 139777017827920
	139762938174016 [label="decoder1.block.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139762938174016 -> 139777017830896
	139777017830896 [label=AccumulateGrad]
	139777017830848 -> 139777017831232
	139762938173936 [label="decoder1.block.4.weight
 (64)" fillcolor=lightblue]
	139762938173936 -> 139777017830848
	139777017830848 [label=AccumulateGrad]
	139777017830224 -> 139777017831232
	139762938174176 [label="decoder1.block.4.bias
 (64)" fillcolor=lightblue]
	139762938174176 -> 139777017830224
	139777017830224 [label=AccumulateGrad]
	139777017830272 -> 139777017828304
	139762938174576 [label="final_conv.weight
 (1, 64, 1, 1)" fillcolor=lightblue]
	139762938174576 -> 139777017830272
	139777017830272 [label=AccumulateGrad]
	139777017828064 -> 139777017828304
	139762938174656 [label="final_conv.bias
 (1)" fillcolor=lightblue]
	139762938174656 -> 139777017828064
	139777017828064 [label=AccumulateGrad]
	139777017828304 -> 139759839991184
}
